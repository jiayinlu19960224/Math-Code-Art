# Visualizing Music with Neural Network

## Brief Description

This tutorial introduces students to a music-conditioned neural visualizer that uses Compositional Pattern-Producing Networks (CPPNs) to create animated artwork driven by audio. The system first learns the overall structure and color palette of a reference image, and then generates video frames whose appearance changes smoothly based on features extracted from the soundtrack.

Students will learn how CPPNs generate visual patterns, how music features can be used as inputs to control the images, and how a network can create animations that change smoothly with the audio.

## Tutorial Lead

Shanmei Wanyan

## Demos

Reference image: examples/target.png
Audio: examples/audio3.wav
Full video: examples/audio3_final.mp4

## References

Chen, Kevin; Shen, Maya; Yin, Kayo; Zheng, Kenneth. “NeuroMV: A Neural Music Visualizer.”
https://kayoyin.github.io/blog/post/neuromv/

Ha, David. “Generating Abstract Patterns with TensorFlow.”
https://blog.otoro.net/2016/03/25/generating-abstract-patterns-with-tensorflow/
